{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "458f2808",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c80a3db",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import os\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a62cf68f",
   "metadata": {},
   "source": [
    "# `torchtune` Exploration\n",
    "\n",
    "I would like to understand the usage of `torchtune`. \n",
    "To start things off, I would like to follow [sesame](https://github.com/SesameAILabs/csm/blob/main/models.py)'s architecture and work on overfitting a pair of conversation from [seamless interaction](https://github.com/facebookresearch/seamless_interaction)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88cbb7d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchtune\n",
    "\n",
    "from torchtune.models import llama3_2\n",
    "import safetensors.torch\n",
    "\n",
    "from transformers import AutoTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6b2c6dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def llama3_2_1B() -> torchtune.modules.transformer.TransformerDecoder:\n",
    "    return llama3_2.llama3_2(\n",
    "        vocab_size=128_256,\n",
    "        num_layers=16,\n",
    "        num_heads=32,\n",
    "        num_kv_heads=8,\n",
    "        embed_dim=2048,\n",
    "        max_seq_len=2048,\n",
    "        intermediate_dim=8192,\n",
    "        attn_dropout=0.0,\n",
    "        norm_eps=1e-5,\n",
    "        rope_base=500_000,\n",
    "        scale_factor=32,\n",
    "    )\n",
    "\n",
    "\n",
    "def llama3_2_100M() -> torchtune.modules.transformer.TransformerDecoder:\n",
    "    return llama3_2.llama3_2(\n",
    "        vocab_size=128_256,\n",
    "        num_layers=4,\n",
    "        num_heads=8,\n",
    "        num_kv_heads=2,\n",
    "        embed_dim=1024,\n",
    "        max_seq_len=2048,\n",
    "        intermediate_dim=8192,\n",
    "        attn_dropout=0.0,\n",
    "        norm_eps=1e-5,\n",
    "        rope_base=500_000,\n",
    "        scale_factor=32,\n",
    "    )\n",
    "\n",
    "\n",
    "FLAVORS = {\n",
    "    \"llama-1B\": llama3_2_1B,\n",
    "    \"llama-100M\": llama3_2_100M,\n",
    "}\n",
    "\n",
    "\n",
    "def _prepare_transformer(model):\n",
    "    embed_dim = model.tok_embeddings.embedding_dim\n",
    "    # model.tok_embeddings = nn.Identity()\n",
    "    model.output = nn.Identity()\n",
    "    return model, embed_dim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "546ba776",
   "metadata": {},
   "outputs": [],
   "source": [
    "llama, embed_dim = _prepare_transformer(FLAVORS[\"llama-100M\"]())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41c5e6cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer_name = \"meta-llama/Llama-3.2-1B\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    tokenizer_name,\n",
    "    token=os.environ[\"HUGGINGFACE_HUB_TOKEN\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f49c8a63",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_hf_to_torchtune(hf_state_dict):\n",
    "    \"\"\"Convert HuggingFace Llama state dict keys to torchtune format.\"\"\"\n",
    "    torchtune_state_dict = {}\n",
    "\n",
    "    key_mapping = {\n",
    "        \"model.embed_tokens.weight\": \"tok_embeddings.weight\",\n",
    "        \"model.norm.weight\": \"norm.scale\",\n",
    "    }\n",
    "\n",
    "    for i in range(16):  # 16 layers for Llama-3.2-1B\n",
    "        key_mapping.update(\n",
    "            {\n",
    "                f\"model.layers.{i}.self_attn.q_proj.weight\": f\"layers.{i}.attn.q_proj.weight\",\n",
    "                f\"model.layers.{i}.self_attn.k_proj.weight\": f\"layers.{i}.attn.k_proj.weight\",\n",
    "                f\"model.layers.{i}.self_attn.v_proj.weight\": f\"layers.{i}.attn.v_proj.weight\",\n",
    "                f\"model.layers.{i}.self_attn.o_proj.weight\": f\"layers.{i}.attn.output_proj.weight\",\n",
    "                f\"model.layers.{i}.mlp.gate_proj.weight\": f\"layers.{i}.mlp.w1.weight\",\n",
    "                f\"model.layers.{i}.mlp.down_proj.weight\": f\"layers.{i}.mlp.w2.weight\",\n",
    "                f\"model.layers.{i}.mlp.up_proj.weight\": f\"layers.{i}.mlp.w3.weight\",\n",
    "                f\"model.layers.{i}.input_layernorm.weight\": f\"layers.{i}.sa_norm.scale\",\n",
    "                f\"model.layers.{i}.post_attention_layernorm.weight\": f\"layers.{i}.mlp_norm.scale\",\n",
    "            }\n",
    "        )\n",
    "\n",
    "    for hf_key, value in hf_state_dict.items():\n",
    "        if hf_key in key_mapping:\n",
    "            torchtune_key = key_mapping[hf_key]\n",
    "            torchtune_state_dict[torchtune_key] = value\n",
    "\n",
    "    return torchtune_state_dict\n",
    "\n",
    "\n",
    "llama = llama3_2_1B()\n",
    "\n",
    "# Load the weights from the safetensors file\n",
    "hf_state_dict = safetensors.torch.load_file(\n",
    "    \"/home/henry/model_weights/Llama-3.2-1B-Instruct/model.safetensors\"\n",
    ")\n",
    "\n",
    "# Convert to torchtune format\n",
    "torchtune_state_dict = convert_hf_to_torchtune(hf_state_dict)\n",
    "\n",
    "# Load the converted state dict\n",
    "llama.load_state_dict(torchtune_state_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cd98eb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "type(llama)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "945fd8d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "llama = llama.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2db0c073",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare the input text\n",
    "prompt = \"The capital of Spain is\"\n",
    "inputs = tokenizer(prompt, return_tensors=\"pt\")\n",
    "\n",
    "# Put model in eval mode\n",
    "llama.eval()\n",
    "\n",
    "# Perform forward pass\n",
    "with torch.no_grad():\n",
    "    # Get input ids\n",
    "    input_ids = inputs[\"input_ids\"].to(device)\n",
    "\n",
    "    # Store the original prompt for display\n",
    "    generated_text = prompt\n",
    "\n",
    "    # Perform autoregressive decoding 5 times\n",
    "    for i in range(10):\n",
    "        # Forward pass through the model\n",
    "        outputs = llama(input_ids)\n",
    "\n",
    "        # Get the logits for the last token position\n",
    "        logits = outputs.logits if hasattr(outputs, \"logits\") else outputs\n",
    "        next_token_logits = logits[:, -1, :]\n",
    "\n",
    "        # Get the most likely next token\n",
    "        next_token_id = torch.argmax(next_token_logits, dim=-1)\n",
    "\n",
    "        # Decode the prediction\n",
    "        predicted_token = tokenizer.decode(next_token_id)\n",
    "\n",
    "        # Add the predicted token to our generated text\n",
    "        generated_text += predicted_token\n",
    "\n",
    "        # Append the new token to input_ids for next iteration\n",
    "        input_ids = torch.cat([input_ids, next_token_id.unsqueeze(0)], dim=1)\n",
    "\n",
    "        print(f\"Step {i+1}: Added '{predicted_token}' -> '{generated_text}'\")\n",
    "\n",
    "    print(\"\\nFinal result:\")\n",
    "    print(f\"Input: {prompt}\")\n",
    "    print(f\"Generated: {generated_text}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92a0983e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2d80deb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
